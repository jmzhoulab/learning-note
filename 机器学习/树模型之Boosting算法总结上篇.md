# [树模型之Boosting算法总结上篇](https://www.cnblogs.com/x739400043/p/10098659.html)

上一篇总结了bagging和boosting的区别以及相应的bagging算法，这篇总结boosting对应的算法

# ADAboost

Adaboost算法是boosting算法之一,会更多的关注上一次分类器误分的样本。训练时将样本初始赋一个权重，用这些样本训练一个基分类器，根据这个基分类器对样本的表现调整样本的权重，如果样本分类器正确的话，权重降低，如果样本分类错误的话，权重上升。再用调整过的权重的样本训练基分类器，反复学习知道分类器个数达到指定个数。对预测样本的输出是结合所有基分类器的权重输出。最小化目标是指数损失函数。

## 模型评价(准确率,时间,空间,模型复杂度,过拟合,抗噪声,是否可并行化,调参)

1. 准确率: 模型的实际效果一般, 我自己做多分类的时候选用ADAboost作为二分类效果并没有很好.
2. 时间: 在小数据上, 模型选用简单的树模型, 有深度限制并且不需要大量的树, 所以时间很快
3. 空间: 需要树的数量少, 消耗的空间也就小
4. 模型复杂度:模型稍微优点复杂,主要是样本的权重调整计算公式上.
5. 过拟合: 周志华的Margin理论：
   泛化错误（泛化错误可理解为测试错误） < 训练错误 + 学习算法容量相关项 (1)
   泛化错误 < 训练Margin项 + 学习算法容量相关项 (2)　Margin:对的信心
   泛化错误 < 训练Margin的最小值 + 学习算法容量相关项 (3) 信心最不足的那个
   泛化错误 < 训练Margin的某个值 + 学习算法容量相关项 (3) 某个信心值
   现实的经验例子也证明了这一点，AdaBoost不容易过拟合。随着Margin变大，泛化误差会收敛
6. 抗噪声: 容易受到噪声的污染
7. 是否可并行化: 模型不可并行化, 特征可以并行化
8. 调参: 树的个数和缩减率,调参是比较容易的.

## 优点

1. 不改变训练数据，只是改变训练样本的权重分布，可以重复使用数据
2. 需要的树模型个数比bagging少，训练时间更短，内存需求更小
3. 吸收bagging的优点，在sklearn中可以随机选择样本和特征

## 缺点

1. 过分关注容易错误分类的样本，如果错误分类样本是噪声点，那么算法容易产生过拟合。
2. 需要调整每棵树的参数，确保单棵树不会过拟合(这也是所有Boosting算法的通病)

------

# GBDT

GBDT是一种迭代的决策回归树算法,也是一种典型的Boosting算法。原始的工作流程是：初始化**CART回归树**，针对每个样本计算残差，将这个残差作为新的真实值，用新的数据去训练下一个CART树。不断迭代直到决策树的个数达到预定义的数量停止，结合所有基分类器的输出作为最后结果的输出。(现在已经有了一个模型F,但是觉得它做的不够好,希望在他的基础上得到一个更好的模型,所以就新加一个模型h(x), 这样最后得到的总结果就是F(x)+h(x))。用缩减率改进的工作流程:认为每次走一小步比走一大步更能获得好的效果，更能防止过拟合。工作流程：仍然以残差作为学习目标，但是每次只是用step*残差逐步逼近目标，step一般是0.001-0.1.和神经网络的的学习率是一样的。本质上缩减是给每棵树设置了一个权重系数。所以能取得良好的效果。

## QA

- 为什么要使用回归决策树?
  GBDT的核心在于累加所有树的结果.
   分类树用于分类, 最后的结果是类别, 如果男女, 这样累加类别是没有意义的.
   回归树用于预测数值, 最后的结果是实数, 累加起来依然是具有实际意义的.
- 分类树和回归树的区别是什么?
    分类树使用信息增益/信息增益率/基尼指数来划分节点, 中间会穷举所有特征的所有阈值, 最后选择一个合适的划分特征,最后根据叶子节点的投票确定预测样本的类别
    回归树使用最小化均方差划分节点, 中间会穷举所有特征的所有划分点, 最后根据叶子节点的样本均值作为预测样本的回归预测值.
- 为什么将残差做为真实值？
    GBDT算法本质是一种加法模型，它的目标函数是MSE，导数是𝑦𝑖~−𝑦𝑖yi~−yi。所以残差就是下一步的最优化的反方向。
    每一步的残差计算其实变相的增大了分错样本的权重，而已经分对的样本则都趋向于0。这样后面就更加专注于那些分错的样本。

## 模型评价(准确率,时间,空间,模型复杂度, 过拟合,抗噪声, 是否可并行化,调参)

1. 准确率: 针对分类和回归任务都有很好的性能
2. 时间: 小型的数据上,不需要训练大量的树, 模型也比较简单, 时间复杂度较小。但是在大数据上, 需要对数据进行无数次的遍历，不能用类似mini batch的方式来训练。如果想要速度，就需要把数据都预加载在内存中，但这样数据就会受限于内存的大小；如果想要训练更多的数据，就要使用外存版本的决策树算法。虽然外存算法也有较多优化，SSD也在普及，但在频繁的IO下，速度还是比较慢的。
3. 空间: 需要较少
4. 模型复杂度: 模型最后的公式很简单
5. 过拟合: 和ADAboost的过拟合效果差不多.
6. 抗噪声: 引入样本随机和列随机后可以抵抗噪声带来的影响
7. 是否可并行化:主要是在生成每一颗树的时候：并行的计算最佳分裂点。每个节点负责一些特征.
8. 调参:树的棵树,缩减率, 采样率(不放回采样), 损失函数

## Adaboost vs. GBDT

本质来说唯一的差别是Adaboost算法使用log损失函数，GBDT使用平方差损失函数




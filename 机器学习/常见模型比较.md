| 比较的模型            | 相同点                                                       | 不同点 |
| --------------------- | ------------------------------------------------------------ | ------ |
| 逻辑回归  VS  **SVM** | LR和SVM都是分类算法<br>LR和SVM都是监督学习算法<br>LR和SVM都是判别模型<br>如果不不考虑核函数，LR和SVM都是线性分类算法 |        |
|                       |                                                              |        |
|                       |                                                              |        |
|                       |                                                              |        |



# 1. 逻辑回归  VS  **SVM**

## 1.1 相同点

- LR和SVM都是分类算法
- LR和SVM都是监督学习算法
- LR和SVM都是判别模型
- 如果不不考虑核函数，LR和SVM都是线性分类算法

## 1.2 不同点

- **LR**采⽤用**log**损失，**SVM**采⽤用合⻚页**(hinge)**损失

- **LR**对异常值敏敏感，**SVM**对异常值不不敏敏感
- 计算复杂度不不同。对于海海量量数据，**SVM**的效率较低，**LR**效率⽐比较⾼高
- 对⾮非线性问题的处理理⽅方式不不同
- **SVM**的损失函数就⾃自带正则
- SVM⾃自带结构⻛风险最⼩小化，LR则是经验⻛风险最⼩小化。
- SVM会用核函数而LR一般不用核函数。



# 2. GBDT VS XGBoost

## 1.1 相同点

- GBDT 和 XGBoost都是分类算法
- GBDT 和 XGBoost都是监督学习算法
- GBDT 和 XGBoost都属于加法模型，是Boosting 算法

## 1.2 不同点

- GBDT目标函数近似只是一阶泰勒公式展开，而XGBoost使用二阶泰勒公式展开
- GBDT是没有正则项的，XGBOOST中加入了正则项，基学习为CART时，正则化项与树的叶子节点的数量T和叶子节点的值有关。
- 上面提到CART回归树中寻找最佳分割点的衡量标准是最小化均方差，xgboost寻找分割点的**标准是最大化**，lamda，gama与正则化项相关
- 支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。
- shrinkage（缩减）对每颗子树增加一个参数，使得每颗子树的权重降低，防止过拟合
- 对特征进行降采样，灵感来源于随机森林，除了能降低计算量外，还能防止过拟合。
- 增加处理缺失值的方案（通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向）
- 对每个特征进行分块（block）并排序，使得在寻找最佳分裂点的时候能够并行化计算.这个结构加速了split finding的过程，只需要在建树前排序一次，后面节点分裂时直接根据索引得到梯度信息。这是xgboost比一般GBDT更快的一个重要原因。
- out-of-core 通过将block压缩（block compressoin）并存储到硬盘上，并且通过将block分区到多个硬盘上（block Sharding）实现了更大的IO 读写速度，因此，因为加入了硬盘存储block读写的部分不仅仅使得xgboost处理大数据量的能力有所提升，并且通过提高IO的吞吐量使得xgboost相比一般实利用这种技术实现大数据计算的框架更快。